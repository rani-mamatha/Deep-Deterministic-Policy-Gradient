# DDPGA-TS: Deep Deterministic Policy Gradient Algorithm for Dynamic Task Scheduling

Implementation of the paper: "Deep Deterministic Policy Gradient Algorithm for Dynamic Task Scheduling in Edge-Cloud Environment Using Reinforcement Learning"

## ğŸ“‹ Overview

This project implements **DDPGA-TS**, a novel reinforcement learning algorithm for dynamic task scheduling in edge-cloud computing environments. The algorithm uses:

- **Conv1D** for local feature extraction
- **GRU** for temporal dependencies learning
- **Attention mechanism** for prediction enhancement
- **Novel pruning strategy** for action space reduction

## ğŸ—ï¸ Project Structure
```
objective 2/
â”‚
â”œâ”€â”€ config/                          # Configuration files
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                    # Environment configurations
â”‚   â””â”€â”€ hyperparameters.py           # Training hyperparameters
â”‚
â”œâ”€â”€ models/                          # Neural network models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ actor_network.py             # Actor network (policy)
â”‚   â”œâ”€â”€ critic_network.py            # Critic network (Q-function)
â”‚   â””â”€â”€ ddpg_agent.py                # DDPG agent implementation
â”‚
â”œâ”€â”€ environment/                     # Edge-Cloud environment
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ job.py                       # Job class
â”‚   â”œâ”€â”€ resource_manager.py          # Resource management
â”‚   â””â”€â”€ edge_cloud_env.py            # Environment implementation
â”‚
â”œâ”€â”€ utils/                           # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ replay_buffer.py             # Experience replay buffer
â”‚   â”œâ”€â”€ noise.py                     # Ornstein-Uhlenbeck noise
â”‚   â”œâ”€â”€ metrics.py                   # Performance metrics tracker
â”‚   â””â”€â”€ pruning.py                   # Action space pruning (Novel!)
â”‚
â”œâ”€â”€ algorithms/                      # Algorithm implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ddpga_ts.py                  # Proposed DDPGA-TS algorithm
â”‚   â”œâ”€â”€ ddpg_nn.py                   # DDPG-NN baseline
â”‚   â””â”€â”€ ddpg_cnn.py                  # DDPG-CNN baseline
â”‚
â”œâ”€â”€ experiments/                     # Training and evaluation scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py                     # Training script
â”‚   â”œâ”€â”€ evaluate.py                  # Evaluation script
â”‚   â””â”€â”€ compare_models.py            # Model comparison
â”‚
â”œâ”€â”€ visualization/                   # Plotting and visualization
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ plot_convergence.py          # Convergence plots
â”‚   â””â”€â”€ plot_performance.py          # Performance comparison plots
â”‚
â”œâ”€â”€ results/                         # Results directory (auto-created)
â”‚   â”œâ”€â”€ small/                       # Small scale results
â”‚   â”œâ”€â”€ medium/                      # Medium scale results
â”‚   â”œâ”€â”€ large/                       # Large scale results
â”‚   â””â”€â”€ plots/                       # Generated plots
â”‚
â”œâ”€â”€ saved_models/                    # Trained models (auto-created)
â”‚   â”œâ”€â”€ DDPGA-TS_Proposed/
â”‚   â”œâ”€â”€ DDPG-NN/
â”‚   â””â”€â”€ DDPG-CNN/
â”‚
â”œâ”€â”€ logs/                            # Training logs (auto-created)
â”‚
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ main.py                          # Main execution script
â””â”€â”€ README.md                        # This file
```

## ğŸš€ Quick Start

### 1. Installation
```bash
# Clone or navigate to the project directory
cd "C:\Users\krake\Downloads\mamatha rani\objective 2"

# Install dependencies
pip install -r requirements.txt
```

### 2. Run Full Experiment (Small Scale)
```bash
python main.py --mode full --scale small
```

This will:
1. Setup directories
2. Train all three models (DDPGA-TS, DDPG-NN, DDPG-CNN)
3. Evaluate all models
4. Generate comparison report
5. Create all visualizations

### 3. Run All Scales
```bash
python main.py --mode full --scale all
```

## ğŸ“Š Environment Scales

| Scale  | Edge Nodes | Cloud Nodes | Jobs      |
|--------|-----------|-------------|-----------|
| Small  | 10        | 10          | 10,000    |
| Medium | 30        | 30          | 100,000   |
| Large  | 50        | 50          | 1,000,000 |

## ğŸ¯ Usage Examples

### Train Only DDPGA-TS on Medium Scale
```bash
python main.py --mode train --model ddpga_ts --scale medium
```

### Train All Models on Large Scale
```bash
python main.py --mode train --scale large
```

### Evaluate Models
```bash
python main.py --mode evaluate --scale small --eval-episodes 20
```

### Generate Visualizations
```bash
python main.py --mode visualize --scale all
```

### Compare Models
```bash
python main.py --mode compare
```

### Use GPU (if available)
```bash
python main.py --mode full --scale small --device cuda
```

## ğŸ“ˆ Performance Metrics

The implementation tracks and compares the following metrics:

1. **Convergence Metrics**
   - Normalized Reward
   - Normalized Training Loss

2. **Performance Metrics**
   - Average Operational Cost
   - Average Rejection Rate
   - Quality of Experience (QoE)

## ğŸ”¬ Key Features

### Novel Contributions (DDPGA-TS)

1. **Action Space Pruning Strategy**
   - Continuously monitors resource utilization
   - Reduces action space by pruning low-utility resources
   - Improves convergence and performance
   - Enhances load balancing

2. **Advanced Neural Architecture**
   - Conv1D for local feature extraction
   - GRU for temporal dependencies
   - Multi-head attention mechanism
   - Better than standard NN and CNN approaches

### Baseline Methods

1. **DDPG-NN**: Standard DDPG with fully connected networks
2. **DDPG-CNN**: DDPG with Conv1D only (no GRU or Attention)

## ğŸ“Š Expected Results

Based on the paper, DDPGA-TS should outperform baselines:

### Small Scale Environment
- **Operational Cost**: DDPGA-TS (1.4) < DDPG-NN (1.9) < DDPG-CNN (2.1)
- **Rejection Rate**: DDPGA-TS (4) < DDPG-NN (15) < DDPG-CNN (16)
- **QoE**: DDPGA-TS (0.62) > DDPG-NN (0.54) > DDPG-CNN (0.48)

### Medium Scale Environment
- **Operational Cost**: DDPGA-TS (1.3) < DDPG-NN (2.1) < DDPG-CNN (2.3)
- **Rejection Rate**: DDPGA-TS (3) < DDPG-NN (17) < DDPG-CNN (20)
- **QoE**: DDPGA-TS (0.69) > DDPG-NN (0.48) > DDPG-CNN (0.40)

### Large Scale Environment
- **Operational Cost**: DDPGA-TS (1.1) < DDPG-NN (2.3) < DDPG-CNN (2.4)
- **Rejection Rate**: DDPGA-TS (2) < DDPG-NN (23) < DDPG-CNN (24)
- **QoE**: DDPGA-TS (0.70) > DDPG-NN (0.40) > DDPG-CNN (0.28)

## ğŸ”§ Configuration

### Hyperparameters (in `config/hyperparameters.py`)
```python
ACTOR_LR = 0.0001           # Actor learning rate
CRITIC_LR = 0.001           # Critic learning rate
GAMMA = 0.99                # Discount factor
TAU = 0.001                 # Soft update parameter
BATCH_SIZE = 128            # Training batch size
BUFFER_SIZE = 100000        # Replay buffer size
MAX_EPISODES = 1000         # Training episodes
```

### Environment Parameters (in `config/config.py`)
```python
BANDWIDTH_EDGE_CLOUD = 1e9  # 1 Gbps
PROPAGATION_TIME_EDGE = 0.005  # 5ms
PROPAGATION_TIME_CLOUD = 0.050  # 50ms
```

## ğŸ“ Output Files

After running experiments, you'll find:

### Results
```
results/
â”œâ”€â”€ small/
â”‚   â”œâ”€â”€ DDPGA-TS_Proposed/metrics.json
â”‚   â”œâ”€â”€ DDPG-NN/metrics.json
â”‚   â””â”€â”€ DDPG-CNN/metrics.json
â”œâ”€â”€ plots/
â”‚   â”œâ”€â”€ normalized_reward_small.png
â”‚   â”œâ”€â”€ normalized_loss_small.png
â”‚   â”œâ”€â”€ operational_cost_comparison.png
â”‚   â”œâ”€â”€ rejection_rate_comparison.png
â”‚   â””â”€â”€ qoe_comparison.png
â””â”€â”€ comparison/
    â”œâ”€â”€ operational_cost.csv
    â”œâ”€â”€ rejection_rate.csv
    â””â”€â”€ qoe.csv
```

### Saved Models
```
saved_models/
â”œâ”€â”€ DDPGA-TS_Proposed/
â”‚   â””â”€â”€ small/best_model.pth
â”œâ”€â”€ DDPG-NN/
â”‚   â””â”€â”€ small/best_model.pth
â””â”€â”€ DDPG-CNN/
    â””â”€â”€ small/best_model.pth
```

## ğŸ› Troubleshooting

### Memory Issues
If you encounter memory issues with large scale:
```bash
# Reduce batch size in config/hyperparameters.py
BATCH_SIZE = 64  # Instead of 128
```

### Slow Training
```bash
# Use GPU if available
python main.py --mode train --scale small --device cuda

# Or reduce number of jobs for testing
# Edit config/config.py and reduce num_jobs
```

### Missing Dependencies
```bash
pip install --upgrade -r requirements.txt
```

## ğŸ“š Algorithm Details

### Reward Function
```
R(j) = Gain(j) - Cost(j)
```

### Gain Function
```
Gain(j) = Î´ Ã— (deadline - RTT)  if RTT â‰¤ deadline
```

### Cost Function
```
Cost(j) = (bandwidth_cost Ã— bandwidth Ã— RTT) + (VM_cost Ã— CPU Ã— RTT)
```

### RTT (Round Trip Time)
- **Edge**: 2 Ã— (5ms + data/bandwidth + processing_time)
- **Cloud**: 2 Ã— (50ms + data/bandwidth + processing_time)

## ğŸ¤ Contributing

This is a research implementation. Feel free to:
- Report issues
- Suggest improvements
- Extend the implementation

## ğŸ“„ Citation

If you use this code, please cite:
```
Deep Deterministic Policy Gradient Algorithm for Dynamic Task Scheduling 
in Edge-Cloud Environment Using Reinforcement Learning
```

## ğŸ“§ Contact

For questions or issues, please open an issue in the repository.

## ğŸ“ Acknowledgments

This implementation is based on the research paper on DDPGA-TS for edge-cloud task scheduling.

## ğŸ“ License

This project is for academic and research purposes.

---

**Happy Experimenting! ğŸš€**